1. 인공지능

인공지능도 하나의 SW 프로그램
인공지능의 90% 이상은 지도학습 (supervised learning) 방식으로 훈련 - 사람이 정답을 알려주면서 진행
사람이 인공지능 구조를 설계하고 다양한 최적화 기법을 적용한 실험을 해야 성능이 개선됨

- 강 인공지능 :
명령 없이도 스스로 판단, 결정하기도 하고 다양한 상황에 유연하게 대처하며 사람과 자연스럽게 대화
매우 범용성을 가져 범 인공지능이라고도 함
- 약 인공지능 :
일상생활에서 만나는 인공지능
제한된 환경에서 구체적인 특정 업무 수행에 사람과 비슷하게 또는 이상으로 성능을 냄
알파고, 체스, 암 진단 등

그러나 범용성과 전문성의 구별이 절대적이지는 않음
범용적이라고 반드시 모든 것을 할 수 있지 않고, 전문적이라고 해서 하나의 작업만 하는 것도 아님

- 인공지능 - 사람이 해야 할 일을 기계가 대신할 수 있는 모든 자동화
- 머신러닝 - 명시적 규칙을 프로그래밍하지 않고 데이터로부터 의사 결정을 위한 패턴을 기계가 스스로 학습
- 딥러닝 - 인공신경망 기반의 모델로, 비정형 데이터로부터 특징 추출 및 판단까지 기계가 한번에 수행 (좁은 의미의 인공지능)

구분 기준 : 특징 추출, 판단 방식을 사람이 하느냐 기계가 하느냐
  .특징 추출 - 문제 해결을 위해 어떤 정보가 유용할 지 중요한 것을 꺼내는 과정
  .판단 방식 - 특징으로부터 인식을 하거나 판단을 모델로 만드는 단계
사람 - 사람 : 규칙 기반 프로그래밍 (AI)
사람 - 기계 : 머신러닝
기계 - 기계 : 딥러닝
규칙 기반 프로그래밍 -> 딥러닝으로 갈수록 사람의 개입이 줄어듬


2. 사람처럼 판단하기

- 정형 데이터 : 
전통적 머신러닝 기반 기술로 다룰 수 있는 데이터
사람이 정제하고 정리한 데이터
안정성은 높으나 체계적으로 구조가 고정되어 있어 유연하지 않음
머신러닝이 이런 데이터에 특화되어 있음
- 비정형 데이터 :  
딥러닝 기반 AI가 다루는 데이터
다양한 형식을 가지는 raw data에 가까운 형태

모라벡의 역설 - 사람에게 쉬운 것이 기계에게는 어렵고, 기계에게 쉬운 것은 사람에게 어렵다.
사람은 사전적 정의나 규칙이 아닌 다양한 사례로부터 귀납적으로 긍정 표현과 부정 표현에 대한 정보를 습득


* 인공신경망
딥러닝은 인간의 사고방식을 기계학습에 반영
컴퓨터공학과 신경과학의 콜라보
사람의 신경세포 뉴런을 인공적으로 구성하고 여러 층을 엮어 기계도 사람처럼 사고하게...
인공뉴런은 이전 뉴런의 데이터를 받아 가중합 연산을 한 뒤, 비선형함수를 적용하여 가공하고 다음 뉴런에게 전달
-> 인간의 지식, 규칙을 전수하지 않고 인간의 사고 방식 자체를 기계에게 알려주고 데이터만 제공
데이터에서 특징을 스스로 찾아야 하므로 양질의 데이터를 다량 확보해야 높은 성능을 얻음
인간 신경계처럼 복잡한 인공신경망을 구성하려면 높은 스펙의 하드웨어 필요


3. Vision AI

* 이미지 인식 방법
인공신경망은 가중합과 비선형 함수로 이루어진 연산을 수행해야 하므로 입력 데이터로 벡터나 행렬 같은 형태가 필요
흑백 이미지는 1과 0으로 표현한 2차원 행렬 데이터로 변환
컬러 이미지는 3차원 텐서 - RGB의 3개 값으로 표현

* CNN - Convolutional Neural Network
이미지는 가로, 세로의 공간적 정보를 담고 있어 특별한 인공신경망 구성이 필요
특징 추출 영역과 태스크 수행 영역으로 구성
특징 추출 :
컨볼루션 연산(특징 찾기)과 풀링 연산(핵심 정보 남기기)이 수행
컨볼루션 연산 - 컨볼루션 필터(또는 커널)가 입력 이미지를 상하좌우로 훑으며 특징이 있는지 찾는 과정, 필터가 여러 개일수록 다양한 특징을 찾을 수 있음
풀링 연산 - 컨볼루션 연산 결과로부터 정보를 추리는 과정, feature map을 훑으며 핵심적 정보만 영역별로 샘플링, 주로 영역 내 가장 큰 값만 남기는 max pooling 방식 적용
-> 보통 컨볼루션과 풀링을 여러 번 반복하여 특징을 추려냄 -> feature learning이라고 함
태스크 수행 : 
특징을 활용하여 목표로 하는 태스크 수행
classification - 입력 이미지를 지정된 k개 클래스 중 하나로 분류
detection - 입력 이미지에서 특정 개체의 좌표를 찾음
segmentation - detection보다 정밀하게 픽셀 단위로 영역 구별


4. 언어 인식

자연어 :
사람들이 일상적으로 쓰는 언어 (한국어, 영어 등)
인공어 : 
의도와 목적에 따라 인공적으로 만든 언어 (프로그래밍 언어 같은 기계어, 전지구인 공용 언어인 에스페란토 등)

언어 - 음성과 문자로 구성, 많은 정보를 함축하여 인코딩된 자료로 사람은 내용을 머리 속에서 상상하 수 있으나 기계는 그런 사전 정보가 없음
이해 - 처리보다 높은 수준으로 처리는 규칙에 따라 수행하나 이해는 맥락 내용에 대한 파악을 전제로 함
자연어 처리(NLP)는 자연어 이해(NLU)에 포함됨
NLP - 언어의 형식, 구문론과 관련된 기능으로 형태소로 쪼개거나 구문 분석 수행
NLU - 표현된 감성 분류, 문서 내 중요 부분 요약 등
-> 전통적 NLP 영역도 딥러닝 적용 시 좋은 성능이 나와 NLP/NLU의 경계가 애매해지고 있음

언어의 데이터화
텍스트로 된 언어 데이터를 인공 뉴런이 계산(가중합+비선형함수)할 수 있는 형태로 변환하려면?
인공신경망은 벡터나 매트릭스 타입이어야 함 (이미지는 2차원의 픽셀, 3차원의 텐서로 했었음...)

Tokenizing
문장을 세부 단위로 쪼개는 작업
쪼개는 방식은 언어의 특징, 수행 태스크, 데이터 특징에 따라 달라짐 (어절, 형태소, 음절, 자소...)
영어는 어절, 중국어는 글자별(한 글자에 뜻이 있어서)로 보통 자름
한국어는 교착어(어근과 접사에 의해 기능이 결정되는 언어)라는 특성상 주로 형태소나 음절 단위로 자름 
정제된 글은 형태소, 채팅/SNS는 맞춤법도 틀린 경우가 많아 음절 단위로 쪼갬

Word Embedding
쪼갠 토큰을 벡터화
토큰을 다차원의 벡터 공간의 한 점에 매핑
- one-hot encoding : 
. 모든 토큰에서 중복을 제거하고 사전 생성
. 사전 길이만큼 벡터를 정의하고 벡터에서 토큰에 해당하는 위치만 1, 나머지는 0으로 벡터 구성
. 모든 토큰을 벡터로 교체
-> 토큰이 많을수록 벡터가 너무 길어지고, 대부분은 0이 들어있음
-> 1,2,3으로 하지 않고 1과 0으로만 하는 것은 모든 토큰이 동등하므로.. 스칼라 값은 수치적으로 왜곡 발생 가능

CBOW, SKIPGRAM
벡터의 길이도 작으면서 정보도 많이 담긴(dense) 방식
사람은 모든 단어의 정확한 의미를 습득하지 않고, 문맥과 위치 등을 통해 대강의 의미를 파악 -> 이 과정을 알고리즘으로 구현
단어(토큰)을 특정 길이를 가진 임의 벡터로 변환 - 벡터 길이는 사람이 지정하며 one-hot encoding보다 훨씬 작음
CBOW :
인공지능에게 문장을 알려주되 중간중간 빈 칸을 만들어 들어갈 단어(토큰)을 유추시킴
SKIPGRAM : 
인공지능에게 단어(토큰) 1개를 알려주고, 주변에 등장할만한 문맥을 만들도록 시킴, 많은 문장을 학습할수록 좋은 벡터가 나옴
-> 유사한 의미를 갖는 토큰은 유사한 벡터값을 가지도록 학습이 됨. 벡터 길이도 작아 저장공간도 효율적, 토큰끼리의 의미 연산도 가능 (벡터 연산)


자연어 이해 태스크
문장/문서 분류 (sentence/document classification) : 
입력받은 텍스트를 지정된 k개 클래스 중 하나로 분류
리뷰 감성분석(긍정/부정), 사용자의 요청을 챗봇이 처리할 기능 중 하나로 매핑하는 의도분류 등
sequence-to-sequence : 
문장/문서를 입력받아 문장을 출력
번역과제, 요약과제, 자유대화 등
질의응답 (question answering) : 
MRC(machine reading comprehension) - 매뉴얼 내에서 가장 답변이 될 가능성이 높은 영역 리턴
IR(information retrieval) - 가장 유사한 과거 질문/답변을 꺼내줌
주로 상담챗봇, 콜센터에서 활용


5. 시계열 데이터 분석

시계열 예측 분석
정형 데이터 분석의 대표적 과제
순차적 시계열 데이터를 활용하여 근미래에 어떤 값이 나타날지 예측
시간 흐름에 따라 변화하는 로그성 데이터를 활용

RNN (Recurrent Neural Network)
과거 데이터를 처리하여 결과를 출력했던 과정 일부를 현 시점의 데이터 처리에 사용
즉 해당 시점의 입력 데이터 뿐만 아니라 이전 시점의 누적된 정보를 가지고 예측 출력

장점 : 
- 과거 처리 내역을 반영하여 더 나은 결정을 할 수 있음
- 가변 길이의 데이터 처리 가능
하루치, 일주일치, 한달치 등 데이터의 기간에 관계없이 같은 내용을 예측한다면 모델을 따로 구성할 필요 없음
- 유연한 구조의 신경망 구성
one to one, one to many, many to one, many to many 등 다양하게 입력 데이터 처리, 누적하여 결과 예측 가능
인코딩 (입력 데이터의 정보를 누적하는 부분), 디코딩 (결과를 출력하는 부분)

단점 : 
- 느린 연산 속도 
이전 시점의 데이터 처리 완료가 필수여서 병렬 처리가 어려움 , 따라서 일반적 딥러닝에서 학습 시 쓰는 GPU 활용 이점도 없음
(정형 데이터의 경우 괜찮은데 텍스트 데이터의 경우 주로 연산 속도 저하가 문제가 됨)
- 불안정한 학습
timestep이 길어지면 반영해야 할 과거 이력이 많아 학습할 값이 폭발하는 Gradient Exploding 발생
또한 너무 멀리 있는 과거 이력은 현재 추론에 거의 영향을 못 미치는 Gradient Vanishing 발생
- 과거 정보를 잘 활용할 수 없음
한 timestep씩 정보를 누적하여 인코딩하여, 먼 과거 정보는 여러 번 압축되고 누적되어 영향을 못 미치게 됨 -> 장기 종속성/의존성 문제(long-term dependency)


성능 보완
LSTM (long-short term memory)
먼 과거 중 중요한 것은 기억하고 불필요한 것은 잊도록 조절
데이터가 길어져도 일반 RNN에 비해 더 좋은 예측이 가능하나 계산과정이 복잡하여 연산속도는 조금 떨어짐
forget gate : 불필요한 부분 제거
input gate : 현재의 정보 (input data)를 얼마나 반영할지 결정
output gate : 현재 시점의 최종 정보를 다음 시점에 얼마나 넘길지 결정

대부분 일반보다 LSTM, GRU 같은 개선된 유닛을 활용

활용 사례
태양광 에너지 발전량 예측
번역 (문장도 단어가 순서대로 등장하는 데이터이므로..)



6. 오버피팅

AI process
offline : -> training pipeline
과거의 데이터를 가공, 정제하여 필요한 부분을 취하거나 라벨(정답)을 붙임 (generate features, collect labels)
머신러닝/딥러닝 알고리즘 선정하여 성능이 좋아질 때까지 반복 실험 (validate & select models, train models)
튜닝 - 실험을 반복하여 문제점이나 성능 해결... 수치 조절 등을 포함
배포할 수 있는 최종 모델 선택 (publish model)
online :
AI 모델을 운영 환경에 띄우고 실제 추론
개발 영역에 가까움


오버피팅(overfitting) & 일반화(generalization) 성능
generalization :
이전에 본 적 없는 데이터(live data)에 대해서도 잘 수행하는 능력
훈련시에만 잘 작동하고 일반화 성능이 떨어지는 모델을 오버피팅이라고 함

training, validation, test
8:1:1 또는 6:2:2 정도로 구분
training set : 머신러닝/딥러닝 모델 학습에 이용하는 데이터, 최적화에 해당
validation set : 모델 튜닝에 도움, 일반화 성능을 판단하여 이어질 실험을 계획하는데 이용, 이 결과를 보고 실험을 개선...
test set : 학습에 관여하지 않은 데이터, 모델의 최종 성능 평가를 위해 떼어놓은 데이터. 모델 성능 비교 시 test data에 대한 점수를 활용

학습 곡선
학습이 진행됨에 따라 모델의 성능을 기록한 그래프
training set에 대해서만 성능이 개선되고 validation set에서는 별 향상이 없으면 오버피팅 발생 시점임

Regularization 
오버피팅을 피하기 위한 모든 전략 - 처음 보는 데이터에 대해서도 잘 맞추기 위함 (목적은 일반화 성능 향상)
데이터 증강 :
더 많은 데이터 확보
기존 데이터를 여러 방식으로 변화를 주어 건수를 늘림 (이미지 좌우 반전, 노이즈 추가, 색상/명암/채도 변화 등)
과도한 변형은 주의
capacity 줄이기 :
모델의 복잡도를 뜻하며 보통 딥러닝 > 머신러닝이고, 신경망 층이 많거나 뉴런 수가 많으면 높아짐
capacity가 높으면 복잡 다양한 패턴을 더 담아낼 순 있으나 과도하면 주어진 데이터를 외우게 될 가능성이 높아짐
태스크의 복잡도와 capacity 관계에 규칙이 없어서 적정 수준을 정하기는 어려움
오버 피팅 경향이 발견된다 싶으면 층 수를 줄이거나 한 층의 뉴런 수를 줄이든가 조치를 취해볼 필요가 있음
조기 종료 (early stopping) :
오버피팅 감지 시 학습 종료 (계획했던 epoch 도달 전 중단)
코드로 구현해도 되지만 기계학습 프레임워크에서 잘 패키징하여 제공
드롭아웃 (dropout) : 
학습 과정에서 일정 비율 p만큼의 노드(인공 뉴런)을 무작위로 끄고 진행
일부 노드가 사라진 상태에서 어떻게든 정답을 맞춰야 하는 모델은 더 강력해진다고 함


7. 전이 학습 (Transfer Learning)

AI 적용 프로젝트의 문제
구체적이지 않으며 불명확한 태스크 - AI는 모든 일의 근본적 해결책이 아니며, 막연한 요구사항보다 구체적이고 명확한 목표를 세우고 필요한 하위 기능을 쪼개어 생각해야 함
적은 데이터, 낮은 품질의 데이터 - 딥러닝은 규칙 없이 데이터를 통해 패턴 학습을 하므로 양질의 다양한 데이터가 필요하지만 보통 건수가 적거나 학습에 부적격
다른 도메인 환경 - 딥러닝 모델은 동일 기능을 수행하더라도 추론 환경이 달라지면 제 기능을 수행하지 못함 (ex. CCTV 화질, 밝기, 카메라 위치 등..)

Transfer Learning
딥러닝 모델 재활용 기법
비슷한 태스크를 다른 도메인에 적용할 때, 태스크를 위한 학습 데이터가 부족할 때 쓰임
Fine-tuning : 더 정교하게 파라미터 튜닝, 이미 적용된 모델이 추론 성능이 떨어지면 새 데이터를 넣어 파라미터를 재학습하여 기존 모델을 보강하는 것(동일 태스크를 위한 모델의 성능 보강)
transfer learning : 모델을 다른 태스크에 적용할 때 활용하는 것
이미 만들어 놓은 모델의 아키텍처를 새 태스크에 맞게 수정한 뒤 새 태스크에 대한 학습 데이터를 이어서 학습 (새 태스크의 데이터가 적어도 괜찮음)

catastropic forgetting
딥러닝 모델이 새 정보를 학습할 때 이전 정보를 잊어버리는 경향 발생
전이(transfer)가 필요 이상으로 과하게 일어난 것...
이를 발생하지 않게 하려면?

보통 딥러닝 모델은 초반에 데이터의 구체적이고 기본적인 특징을, 후반부로 갈수록 태스크를 위한 추상적이고 개념적인 특징을 학습
이미 학습해둔 모델은 해당 데이터의 기본적 특징을 알고 있으므로 transfer learning을 할 땐 기본 특징 학습을 건너뛰고 태스크 학습을 함
레이어 동결(layer freezing) : 
전반부 신경망 층의 파라미터는 학습되지 않게 하고, 후반부의 신경망 층에 대해서만 파라미터 학습
새 태스크가 거의 유사하면 대부분 동결, 많이 다르면 더 앞까지 학습, 다 동결했다가 학습을 진행하면서 후반부부터 동결을 서서히 풀기도 함 (gradual freezing)
discriminative fine-tuning :
층마다 learning rate에 차별화
learning rate는 한번에 인공신경망의 파라미터를 얼마만큼 업데이트할 지 사람이 설정하는 값
전반부는 기본적 특징을 이미 배워놨으니 learning rate를 작게 설정하고 태스크를 위한 후반부에 크게 설정

transfer learning 모델 이용
보통 오픈도메인 데이터에 대해 만들어놓은 모델을 특정 도메인의 태스크에 적용하는 식으로 활용
일반적 내용을 학습한 모델의 사전 지식을 구체적 하위 영역에 활용
예) 이미지가 입력 데이터인 경우 -> 이미지넷(120만장 이미지를 1천개 카테고리로 분류)

자연어 이해의 transfer learning
텍스트는 특히 함축적 데이터라 transfer learning이 매우 효과적
KorQuAD(Korean Question Answering Dataset) - 자사에서 공개한 한국어 위키백과를 소스로 한 오픈도메인 질의응답 데이터


8. AI 사전학습 및 자기주도학습

pre-training (사전학습), pre-trained model (사전학습 모델)
여러 태스크에 활용하기 위해 여러 지식을 두루 두루 학습해놓은 모델
비슷한 태스크의 모델이 없으면 재활용할 수 없으므로 이런 경우는 transfer learning도 어려움
전이 학습을 염두에 두고 다방면으로 활용할 수 있는 모델을 미리 만듦
보통 특정 데이터 타입에 대한 일반적 지식을 학습하는 것을 목표

대규모 데이터 사전학습
어떤 태스크에든 잘 적용하려면 방대한 양의 지식을 고루 학습하는 것이 좋음
보통 대규모의 오픈도메인 데이터에 대해 이루어짐
시각 데이터 사전학습 :
이미지넷 데이터 인식 대회 - 120만 장 이미지를 1000개 카테고리로 분류
구글 youtube-8m - 35만 시간 분량의 610만 개 비디오를 3800개 카테고리로 다중 분류
언어 데이터 사전학습 : 
문맥에 따라 활용되는 단어의 의미, 뉘앙스, 적절한 문체 등을 습득
영어는 공개 데이터가 많지만 한국어나 기타 언어는 데이터 구하기 어려움
위키피디아, 나무위키, 세종말뭉치 등 (뉴스, 리뷰 등도 좋음)

self-supervised learning (자가지도학습)
대규모 데이터에 라벨(정답)까지 달려면 힘듦
supervised - 정답이 달린 데이터로 학습
semi-supervised - 정답이 있는 데이터, 없는 데이터를 섞어서 학습
unsupervised - 정답이 달리지 않은 데이터로 학습
self-supervised는 라벨이 없어도 기계가 시스템적으로 자체 라벨을 만들어서 사용하는 학습 방법 
(사람이 라벨을 만들어주지 않는다는 점에서는 unsupervised, 자기가 라벨을 만들었다는 점에서는 supervised로 볼 수도 있음)
다량의 데이터는 있는데 라벨이 없는 경우에 활용, 주로 사전학습에서 이용

예1) 이미지 데이터
다람쥐와 청설모 분류하기 - 모든 설치류 사진 10만장, 라벨은 없음
원본을 임의 회전시켜 4개 카테고리를 맞추도록 라벨을 기계적으로 부여하고, 이를 맞추는 사전학습 수행
사전학습한 모델을 다람쥐/청설모 데이터로 transfer learning했더니 좋은 성능이 나옴

예2) 텍스트 데이터
사외 전송 메일의 보안 위반 여부 검출 - 모든 사외전송 이메일 10만 건, 그 중 1만 건만 라벨링
업무 관련 키워드를 학습할 수 있도록 메일의 중간 단어를 빈 칸으로 대체하고 단어를 맞추는 사전학습 수행
회사에서 자주 쓰는 키워드가 인식되었고, 이 모델을 transfer learning으로 활용하니 좋은 성능이 나옴

Google BERT (Bidirectional Encoder Representations from Transformers)
self-supervised learning 기법으로 사전학습을 하고 다양한 태스크에 transfer learning을 할 수 있는 대표적인 예
자연어처리 연구 패러다임을 전환한 계기가 된 모델, 사전학습 자체가 주가 되는 모델
언어라는 분야 전반에 걸쳐 지식을 두루 쌓은 거대한 뇌를 사전학습으로 만든다는 개념
사전학습에서 상당한 양의 데이터를 커다란 모델로 학습, 후속 태스크를 위한 transfer learning을 간략하게만 해도 좋은 성능
11개 자연어 처리 과제에서 1위

self-supervised learning을 활용한 사전학습(pre-trained) 모델은 다량의 방대한 지식을 골고루 습득하는 것이 목적
모델 사이즈가 크고 사전학습 규모가 매우 큼
비용 부담이 크지만 잘 만들면 향후 어떤 과제든 적용 가능


9. Active Learning (능동 학습)

데이터는 많지만 인공지능을 학습시킬 데이터는 마련하기 쉽지 않을 때 이용
라벨링 할 인적 자원은 있으나 많은 수의 라벨링을 수행할 수 없을 때 효과적을 하기 위한 기법
태스크가 너무 특수하여 해당 도메인 전문 인력만 할 수 있는데 이런 데이터를 확보할 수 있다면?

학습 데이터 중 모델 성능 향상에 효과적인 데이터를 선별하여 학습
같은 수의 데이터 라벨링을 할 때 성능이 높게 나올 수 있도록 데이터를 선별하면 더 효과적
이 효과적 데이터를 선별하는 방법을 연구하는 것이 active learning ( <-> 주어진 라벨 데이터만 가지고 학습하는 것은 passive learning)

절차 (목표 성능이 나올 때까지 이 방법을 반복 수행)
초기 학습 데이터(라벨 데이터)를 이용해 모델 학습
라벨 없는 데이터에서 모델에 도움이 되는 데이터 선별
선별한 데이터에 사람이 라벨 태깅
선별한 라벨 데이터를 기존 학습 데이터와 병합하고 다시 학습

Query Strategy
active learning의 핵심으로 성능 향상에 효과적인 데이터를 선별하는 방법
모델이 헷갈릴만한 데이터 위주로 추출하여 정답을 알려주고 학습하면 더 정교해질 것
종류
uncertainty sampling - 학습된 모델의 판정 값을 기반으로 선택
query by committee - 여러 개 모델을 동시 학습시키면서 많은 모델이 틀리는 데이터를 선별
expected impact - 데이터가 추가될 때 모델이 가장 많이 변화하는 데이터를 선별
density weighted method - 데이터가 밀집된 지역의 데이터를 선별
core-set approach - 데이터를 최대한 고르게 뽑아서 전체 분포를 대표하도록 선별


Uncertainty sampling
모델이 가장 불확실하다고 생각하는 데이터를 추출하여 라벨링 요청

Query by Committee
여러 AI 모델 간의 의견 불일치를 종합 고려
추론 결과 불일치가 많을수록 헷갈리는 데이터로서 라벨링 대상이 됨


10. Attention Mechanism 

길고 많은 문장을 한꺼번에 받아 번역하려면 인식하기 어려움...

집중
인공신경망 모델이 특정 영역에 더 집중해서 의사결정하는 방법
인공신경망이 입력 데이터의 전체 또는 일부를 되짚어보면서 어떤 부분이 중요한지 집중하는 방식


RNN 적용
어텐션 없이 기본 RNN에서는 단어 하나하나를 압축해서 인코딩하고 있다가 모든 문장이 다 들어오면 영어로 한 단어씩 번역(디코딩)을 수행
디코더가 참고하는 문맥은 입력문이 전부 압축된 하나의 벡터 - 문장을 모두 누적했지만 앞부분은 너무 압축되어 거의 잊어버리게 됨
입력 단어 중 특정 단어에 어텐션 메커니즘을 적용하여 전체 입력을 다시 재조정한 입력 데이터 인코딩 벡터를 만듦


어텐션 스코어
인공신경망 모델이 각 인코딩 timestep마다 계산된 특징을 가지고 자동으로 계산하는 0~1 사이의 값
각 단어에 대한 주의 집중 가중치...

컨텍스트 벡터
어텐션 스코어를 구한 뒤 현재 디코딩할 단어와의 관련성을 반영하여 다시 입력 문장을 인코딩한 것
중요도에 따라 전체 문맥의 정보를 잘 반영했다고 하여 컨텍스트 벡터라고 함
중요한 것은 어텐션 메커니즘은 매번 디코딩마다 직전 단계의 벡터 뿐만 아니라 과거의 모든 데이터의 특징을 고려한다는 것
또한, 딥러닝 모델이 스스로 집중할 영역을 파악한다는 것 (사람이 알려주지 않아도)

XAI(eXplainable AI) 와 어텐션
어텐션 메커니즘은 기계가 중요하게 생각하는 부분을 우리에게 알려주는 역할도 함
설명/해석 가능한 인공지능의 기능으로 이 영역만 따로 연구할 정도로 인공지능의 추론 결과를 해석하는 것은 중요함...

이미지에서의 어텐션
딥러닝 모델이 입력 이미지에 대한 설명문을 생성하는 과제
모델이 각 단어를 생성할  때 이미지의 어떤 영역을 집중해서 보았는지 어텐션 스코어를 확인할 수 있었음

Transformer
어텐션만으로 이루어진 인공신경망
입력 데이터끼리의 self-attention을 통해 상호 정보교환을 수행하는 것이 특징
문장 내 단어들이 서로 정보를 파악하여 관계, 문맥을 더 잘 파악
특히 자연어 이해에서 인공신경망 발전에 큰 획
순차적 계산이 필요없어 RNN보다 빠르고 맥락 파악을 잘하고, CNN처럼 일부씩만 보지 않고 전 영역을 아우름
대신 모델이 커지고 고사양 하드웨어가 필요하여 경량화 방안이 연구되고 있음


11. Auto ML (Automated Machine Learning)

AI 모델을 만들기 위해 실험을 반복할 때 튜닝의 장벽이 있음
모델 구조, 신경망 층 수, 뉴런 수, learning rate, 데이터 배치 크기 ....
사람이 결정해야 할 설정이 매우 많음

AutoML
특정 태스크를 위한 모델 학습에 한하여 사람이 실험에 개입하지 않아도 AI 스스로가 반복실험을 통해 성능을 개선하는 것
역할
feature engineering 자동화 - 데이터로부터 중요한 특징을 선택하고 인코딩하는 방식 (딥러닝 모델은 비정형 데이터를 깊은 인공신경망에 태워 자동으로 특징을 추출하므로 설명 생략)
하이퍼파라미터 자동 탐색 - 모델 학습에 필요한 설정을 자동 탐색
아키텍처 탐색 - 모델 구조 자체를 더 효율적인 방향으로 찾음

하이퍼파라미터 탐색 자동화
에폭, 모멘텀, 컨볼루션 필터 수 등등...
많은 경우 딥러닝 학습 프레임워크에서 기본적으로 잘 작동하는 설정을 제공
실험 결과에 따라 조금씩 튜닝 필요 - 여러 파라미터의 조합을 찾는 시도...
그리드 서치 : 
최적화할 파라미터 값 구간을 일정 단위로 나눈 뒤, 각 단위 조합을 테스트하여 가장 높은 성능을 낸 조합 선택
최적화 대상 파라미터가 많으면 경우의 수가 기하급수적으로 늘어나 탐색에 시간 소요
불필요한 탐색에 시간 허비도 있음
랜덤 서치 : 
랜덤하게 파라미터 조합을 테스트, 그리드서치에 비해 비교적 빠르게 최적 조합을 찾음
meta learner : 
하이퍼파라미터를 모델을 통해 탐색
RNN와 강화학습을 사용하여 최적의 파라미터 조합 탐색
meta learner의 파라미터 조합 -> 이 조합으로 모델 학습, 결과 전달 -> meta learner가 새 조합 찾음 -> AI 모델은 다시 학습
meta learner가 수행하는 학습을 메타 학습 또는 learn to learn이라고도 함

아키텍처 탐색 자동화
사람이 어떤 방식으로 모델 구조를 짤 지 생각하지 않아도 자동 탐색을 통해 최적 구조를 찾음
딥러닝은 인공신경망을 활용해서 NAS(Neural Architecture Search)라고 함
meta learner와 learner(AI 모델)로 이루어져 meta learner는 어떤 신경망을 만들면 좋은지 아키텍처 구성을 고민
역시 RNN과 강화학습을 접목한 형식으로 구성

AutoML 특징
사람이 하나하나 고민하여 하이퍼파라미터를 튜닝할 필요 없이 최적의 환경을 결정해줌
일반적으로 사람이 고민한 모델 이상의 성능을 냄
기존 NAS는 meta learner가 탐색할 아키텍처 공간이 한정되어 있는데, 이 제약을 없애고 랜덤 그래프 생성 방법론을 기반으로 진행하자 기존 구성 방법의 틀을 완전히 깬 모델이 만들어지기도 함
사람의 노력은 적으나 기계가 다양한 시도를 하도록 오랜 시간과 고사양 하드웨어가 필요
leaner와 meta learner가 동시다발적으로 학습해야 하니까...
따라서 AutoML 서비스를 제공하는 업체가 있음 (CSP)

Google AutoML
구글 계정을 만들고 크레딧을 지불하여 이용
제공 기능 - 이미지의 분류/객체 탐지, 동영상의 분류/객체 추적, 자연어의 분류/객체명인식/감정분류/번역, 정형 테이블 데이터의 회귀/분류
엣지 레벨(엣지 기기 탑재)과 클라우드 레벨(API 형식)으로 제공
Google AutoML로 학습된 모델은 어떤 하이퍼파라미터를 사용하고, 어떤 네트워크 아키텍처를 가졌는지 알 수 없음
비용을 지불하고 데이터에 대해 최적으로 맞춰진 모델을 이용할 수 있을 뿐

12. XAI

인공지능의 한계
룰 기반 모델이나 머신러닝과 달리 딥러닝 모델은 사람이 결과를 해석하기 어려움
데이터가 충분하면 룰기반/머신러닝기반 모델보다 성능이 좋을 수 있으나 왜 그런 결과를 도출했는지 사람의 해석 여지가 부족
법, 의료, 국가 보안, 신용 등 민감한 주제와 관련된 경우라면 문제가 심각해짐
-> 인공지능에 설명력을 부여하는 연구 분야 생김

XAI의 필요성
딥러닝 모델에 설명력을 부여하는 것은 사용자가 모델의 결정을 이해하고 결과의 타당성을 확인할 수 있게 해줌
모델에 설명 가능한 근거와 해석력을 부여해서 투명성, 신뢰성을 확보하는 것이 목적

XAI를 위한 접근법
1) AI 모델에 설명할 수 있는 모듈을 덧붙이는 방식
어텐션 메커니즘 활용 : 
딥러닝 모델이 결정을 내릴 때 입력 데이터의 어떤 부분에 집중해서 판단했는지 시각화
설명하는 법 학습하기 (learn to explain) :
딥러닝 모델에 RNN 모듈 등을 붙여 인간이 이해할 수 있는 방식의 설명을 생성
어떻게 판단을 결정했는지 문장을 작성, 한계가 있어 널리 활용되지는 않음
모듈러 네트워크 방식 :
딥러닝 모델이 해석 가능한 모듈 구성요소로 이루어진 경우, 판정 결과가 어떤 모듈 경로를 따라 연산되는지 파악
Feature Identification : 
설명가능한 특징을 학습한 노드를 찾아 그 특징에 설명 라벨을 붙임

2) 설명력 있는 모델을 만듦
예)의사결정나무, 선형회귀분석
딥러닝 모델에서는 적용할 수 없음

3) 다른 모델을 활용하여 유추
인공신경망처럼 복잡한 블랙박스 모델의 일부분을 설명
일부 영역의 데이터를 활용해 설명력이 좋은 모델을 별도로 만들어 학습
단순하여 사람이 해석하기도 쉬움
LIME, SP-LIME 기술이 이에 해당




